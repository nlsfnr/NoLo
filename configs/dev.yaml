datasets: [
  [[wikitext, wikitext-103-v1], {split: train}],  # 0.7 GB
  # [[c4, en], {split: train}],  # 305 GB
  # [[scientific_papers, arxiv], {split: train}, 'article'],  # 11.5 GB
  # [[scientific_papers, pubmed], {split: train}, 'article'],  # 6.7 GB
]
# dataset_weights: [0.7, 305, 6.7]
dataset_weights: [1.]
shuffle_buffer_size: 5000
dataset_buffer_size: 1000

batch_size: 32
sequence_length: 32
min_tokens_per_sequence: 24
tokenizer_path: './tokenizers/wcb-4k.json'
tokenizer_max_training_samples: 100000
tokenizer_min_token_frequency: 100
vocab_size: 4096

lr_min: 0.00006
lr_max: 0.0006
lr_warmup_steps: 250
lr_decay_steps: 10000
gradient_accumulation_steps: 2
gradient_clip_norm: 0.5
adam_b1: 0.9
adam_b2: 0.95
label_smoothing: 0.1
telemetry_interval: 10
checkpoint_interval: 100
